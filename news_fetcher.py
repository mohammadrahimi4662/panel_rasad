import requests
from bs4 import BeautifulSoup
from database import SessionLocal, News
import datetime
from datetime import timezone
from dateutil import parser as date_parser
from urllib.parse import urlparse, urlunparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import openai
import os
import re

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù…Ù‡Ù… Ø§Ø² IRNA Ø¨Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

def fetch_irna_top_news():
    url = 'https://www.irna.ir/'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    news_list = []
    all_news_items = []
    for item in soup.select('div.top-news a')[:10]:
        title = item.get_text(strip=True)
        link = item['href']
        if not link.startswith('http'):
            link = 'https://www.irna.ir' + link
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
        is_duplicate = False
        for existing in all_news_items:
            if existing['title'] == title or existing['url'] == link:
                is_duplicate = True
                break
        if not is_duplicate:
            all_news_items.append({'title': title, 'url': link})
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± IRNA: {len(all_news_items)}")
    
    for i, news_item in enumerate(all_news_items):
        print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± IRNA {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
        summary = ""
        try:
            summary = extract_irna_content_with_summary(news_item['url'], news_item['title'])
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ IRNA: {e}")
            summary = ""
        
        today = datetime.datetime.now(timezone.utc)
        news_list.append({
            'title': news_item['title'],
            'url': news_item['url'],
            'agency': 'IRNA',
            'published_at': today,
            'summary': summary
        })
        print(f"Ø®Ø¨Ø± IRNA {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
    
    print(f"\nIRNA news count: {len(news_list)}")
    return news_list

def extract_irna_content(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² IRNA"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        content_parts = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ IRNA
        selectors = [
            'div.news-content p',
            'div.news-text p',
            'div.content p',
            'article p'
        ]
        
        for selector in selectors:
            paragraphs = soup.select(selector)
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 30 and len(text) < 1000:
                    content_parts.append(text)
                if len(content_parts) >= 5:
                    break
            if len(content_parts) >= 5:
                break
        
        # ØªØ±Ú©ÛŒØ¨ Ù…Ø­ØªÙˆØ§
        if content_parts:
            full_content = ' '.join(content_parts)
            if len(full_content) > 2000:
                full_content = full_content[:2000] + "..."
            return full_content
        else:
            return ""
            
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ IRNA: {e}")
        return ""

def extract_irna_content_with_summary(url, title):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ IRNA Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ ChatGPT"""
    content = extract_irna_content(url)
    if content:
        # IRNA Ø±ÙˆØªÛŒØªØ± Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø² ChatGPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        return get_chatgpt_summary(content, title)
    return ""

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± BBC ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

def fetch_bbc_persian_news():
    try:
        url = 'https://www.bbc.com/persian/topics/ckdxnwvwwjnt'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        all_news_items = []
        selectors = [
            'ul[data-testid="topic-promos"] > li h2 a',
            'article h2 a',
            'div[data-testid="card-headline"] a'
        ]
        for selector in selectors:
            for a_tag in soup.select(selector):
                title = a_tag.get_text(strip=True)
                if not title:
                    continue
                link = a_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.bbc.com' + link
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 15:
                    break
            if len(all_news_items) >= 15:
                break
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± BBC: {len(all_news_items)}")
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± BBC {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø±
                summary = extract_bbc_content_with_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ BBC: {e}")
                summary = ""
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'BBC',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± BBC {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        print(f"\nBBC news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± BBC: {e}")
        return []

def extract_bbc_content(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² BBC"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡
        content_parts = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± div Ø¨Ø§ Ú©Ù„Ø§Ø³ bbc-4wucq3 ebmt73l0
        main_content_divs = soup.find_all('div', class_='bbc-4wucq3 ebmt73l0')
        for div in main_content_divs:
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± p Ø¨Ø§ Ú©Ù„Ø§Ø³ bbc-1gjryo4 e17g058b0
            paragraphs = div.find_all('p', class_='bbc-1gjryo4 e17g058b0')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 20:  # ÙÙ‚Ø· Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±
                    content_parts.append(text)
        
        # Ø§Ú¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if not content_parts:
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ØªÙ…Ø§Ù… p Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
            paragraphs = soup.find_all('p')
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 50 and len(text) < 1000:  # Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙˆØ³Ø·
                    content_parts.append(text)
                if len(content_parts) >= 5:  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                    break
        
        # ØªØ±Ú©ÛŒØ¨ Ù…Ø­ØªÙˆØ§
        if content_parts:
            full_content = ' '.join(content_parts)
            # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨Ø§Ø´Ø¯
            if len(full_content) > 2000:
                full_content = full_content[:2000] + "..."
            return full_content
        else:
            return ""
            
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ BBC: {e}")
        return ""

def extract_bbc_content_with_summary(url, title):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ BBC Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ ChatGPT"""
    content = extract_bbc_content(url)
    if content:
        # BBC Ø±ÙˆØªÛŒØªØ± Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø² ChatGPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        return get_chatgpt_summary(content, title)
    return ""

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§ÛŒØ±Ø§Ù† Ø§ÛŒÙ†ØªØ±Ù†Ø´Ù†Ø§Ù„ Ø¨Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

def fetch_iranintl_news():
    try:
        url = 'https://www.iranintl.com/iran'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        all_news_items = []
        selectors = [
            'article h3',
            'div.TopicCluster-module-scss-module__RZ03fG__featured article h3',
            'div.TopicCluster-module-scss-module__RZ03fG__additionalItem article h3',
            'div.topic__grid__item article h3'
        ]
        for selector in selectors:
            for title_tag in soup.select(selector):
                if not title_tag:
                    continue
                title = title_tag.get_text(strip=True)
                if not title:
                    continue
                article = title_tag.find_parent('article')
                if not article:
                    continue
                link_tag = article.select_one('a')
                if not link_tag:
                    continue
                link = link_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.iranintl.com' + link
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 15:
                    break
            if len(all_news_items) >= 15:
                break
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± IranIntl: {len(all_news_items)}")
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± IranIntl {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                summary = extract_iranintl_content_with_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ IranIntl: {e}")
                summary = ""
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'IranIntl',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± IranIntl {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        print(f"\nIranIntl news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± IranIntl: {e}")
        return []

def extract_iranintl_content(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² IranIntl"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        content_parts = []
        lead_paragraph = ""
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ IranIntl
        selectors = [
            'div.article-content p',
            'div.content p',
            'article p',
            'div.article-body p'
        ]
        
        # Ø§Ø¨ØªØ¯Ø§ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ø±ÙˆØªÛŒØªØ± (Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§ÙˆÙ„) Ø¨Ú¯Ø±Ø¯
        for selector in selectors:
            paragraphs = soup.select(selector)
            if paragraphs:
                # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§ÙˆÙ„ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø±ÙˆØªÛŒØªØ± Ø§Ø³Øª
                lead_text = paragraphs[0].get_text(strip=True)
                if lead_text and len(lead_text) > 50 and len(lead_text) < 300:
                    lead_paragraph = lead_text
                    print(f"âœ… Ø±ÙˆØªÛŒØªØ± IranIntl Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {lead_text[:100]}...")
                    break
        
        # Ø§Ú¯Ø± Ø±ÙˆØªÛŒØªØ± Ù¾ÛŒØ¯Ø§ Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
        if lead_paragraph:
            return lead_paragraph
        
        # Ø§Ú¯Ø± Ø±ÙˆØªÛŒØªØ± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² ChatGPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        for selector in selectors:
            paragraphs = soup.select(selector)
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 30 and len(text) < 1000:
                    content_parts.append(text)
                if len(content_parts) >= 5:
                    break
            if len(content_parts) >= 5:
                break
        
        # ØªØ±Ú©ÛŒØ¨ Ù…Ø­ØªÙˆØ§
        if content_parts:
            full_content = ' '.join(content_parts)
            if len(full_content) > 2000:
                full_content = full_content[:2000] + "..."
            return full_content
        else:
            return ""
            
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ IranIntl: {e}")
        return ""

def extract_iranintl_content_with_summary(url, title):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ IranIntl Ùˆ ØªØ´Ø®ÛŒØµ Ø±ÙˆØªÛŒØªØ±"""
    content = extract_iranintl_content(url)
    if content:
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø±ÙˆØªÛŒØªØ± Ø§Ø³Øª ÛŒØ§ Ù†Ù‡
        if len(content) < 300 and not content.endswith("..."):
            # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø±ÙˆØªÛŒØªØ± Ø§Ø³Øª
            print("âœ… Ø§Ø² Ø±ÙˆØªÛŒØªØ± IranIntl Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
            return content
        else:
            # Ø§Ø² ChatGPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            print("ğŸ”„ Ø§Ø² ChatGPT Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
            return get_chatgpt_summary(content, title)
    return ""

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ISNA
def fetch_isna_news():
    try:
        url = 'https://www.isna.ir/'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        summarizer = PersianSummarizer()
        all_news_items = []
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø±
        selectors = [
            'div.news-list h3 a',
            'div.top-news h3 a',
            'div.latest-news h3 a',
            'article h3 a'
        ]
        
        for selector in selectors:
            for a_tag in soup.select(selector):
                title = a_tag.get_text(strip=True)
                if not title or len(title) < 10:
                    continue
                link = a_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.isna.ir' + link
                
                # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 10:
                    break
            if len(all_news_items) >= 10:
                break
        
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± ISNA: {len(all_news_items)}")
        
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± ISNA {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                summary = extract_isna_content_with_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ISNA: {e}")
                summary = ""
            
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'ISNA',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± ISNA {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        
        print(f"\nISNA news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ISNA: {e}")
        return []

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ØªØ³Ù†ÛŒÙ…
def fetch_tasnim_news():
    try:
        url = 'https://www.tasnimnews.com/'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        summarizer = PersianSummarizer()
        all_news_items = []
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø±
        selectors = [
            'div.news-list h3 a',
            'div.top-news h3 a',
            'div.latest-news h3 a',
            'article h3 a',
            'div.news-item h3 a'
        ]
        
        for selector in selectors:
            for a_tag in soup.select(selector):
                title = a_tag.get_text(strip=True)
                if not title or len(title) < 10:
                    continue
                link = a_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.tasnimnews.com' + link
                
                # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 10:
                    break
            if len(all_news_items) >= 10:
                break
        
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± Tasnim: {len(all_news_items)}")
        
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± Tasnim {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                summary = extract_tasnim_content_with_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Tasnim: {e}")
                summary = ""
            
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'Tasnim',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± Tasnim {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        
        print(f"\nTasnim news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Tasnim: {e}")
        return []

def extract_isna_content(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² ISNA"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        content_parts = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ ISNA
        selectors = [
            'div.news-content p',
            'div.news-text p',
            'div.content p',
            'article p',
            'div.news-body p'
        ]
        
        for selector in selectors:
            paragraphs = soup.select(selector)
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 30 and len(text) < 1000:
                    content_parts.append(text)
                if len(content_parts) >= 5:
                    break
            if len(content_parts) >= 5:
                break
        
        # ØªØ±Ú©ÛŒØ¨ Ù…Ø­ØªÙˆØ§
        if content_parts:
            full_content = ' '.join(content_parts)
            if len(full_content) > 2000:
                full_content = full_content[:2000] + "..."
            return full_content
        else:
            return ""
            
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ISNA: {e}")
        return ""

def extract_isna_content_with_summary(url, title):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ISNA Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ ChatGPT"""
    content = extract_isna_content(url)
    if content:
        # ISNA Ø±ÙˆØªÛŒØªØ± Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø² ChatGPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        return get_chatgpt_summary(content, title)
    return ""

def extract_tasnim_content(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ø®Ø¨Ø± Ø§Ø² Tasnim"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        content_parts = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Tasnim
        selectors = [
            'div.news-content p',
            'div.news-text p',
            'div.content p',
            'article p',
            'div.news-body p'
        ]
        
        for selector in selectors:
            paragraphs = soup.select(selector)
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 30 and len(text) < 1000:
                    content_parts.append(text)
                if len(content_parts) >= 5:
                    break
            if len(content_parts) >= 5:
                break
        
        # ØªØ±Ú©ÛŒØ¨ Ù…Ø­ØªÙˆØ§
        if content_parts:
            full_content = ' '.join(content_parts)
            if len(full_content) > 2000:
                full_content = full_content[:2000] + "..."
            return full_content
        else:
            return ""
            
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Tasnim: {e}")
        return ""

def extract_tasnim_content_with_summary(url, title):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Tasnim Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ ChatGPT"""
    content = extract_tasnim_content(url)
    if content:
        # Tasnim Ø±ÙˆØªÛŒØªØ± Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§Ø² ChatGPT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        return get_chatgpt_summary(content, title)
    return ""

def get_chatgpt_summary(text, title):
    """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø§Ø² ChatGPT"""
    try:
        # ØªÙ†Ø¸ÛŒÙ… API key (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("âš ï¸ OPENAI_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§Ø² Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
            return text[:500] + "..." if len(text) > 500 else text
        
        openai.api_key = api_key
        
        prompt = f"""
        Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±: {title}
        
        Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø®Ø¨Ø±:
        {text}
        
        Ù„Ø·ÙØ§Ù‹ Ø§ÛŒÙ† Ø®Ø¨Ø± Ø±Ø§ Ø¯Ø± ÛŒÚ© Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± Ø®Ù„Ø§ØµÙ‡ Ú©Ù†ÛŒØ¯. 
        Ø®Ù„Ø§ØµÙ‡ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ù†Ú©Ø§Øª Ø§ØµÙ„ÛŒ Ùˆ Ù…Ù‡Ù… Ø®Ø¨Ø± Ø¨Ø§Ø´Ø¯ Ùˆ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´ÙˆØ¯.
        Ø­Ø¯Ø§Ú©Ø«Ø± 200 Ú©Ù„Ù…Ù‡ Ø¨Ø§Ø´Ø¯.
        """
        
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Ø´Ù…Ø§ ÛŒÚ© Ø®Ø¨Ø±Ù†Ú¯Ø§Ø± Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø±Ø§ Ø®Ù„Ø§ØµÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300,
            temperature=0.7
        )
        
        summary = response.choices[0].message.content.strip()
        return summary
        
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø§Ø² ChatGPT: {e}")
        # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ø§Ø² Ù…ØªÙ† Ú©ÙˆØªØ§Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        return text[:500] + "..." if len(text) > 500 else text

def normalize_text(text):
    """Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
    if not text:
        return ""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ùˆ Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù†
    text = text.strip().lower()
    text = re.sub(r'[^\w\s]', '', text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
    text = re.sub(r'\s+', ' ', text)     # ØªØ¨Ø¯ÛŒÙ„ Ú†Ù†Ø¯ÛŒÙ† ÙØ§ØµÙ„Ù‡ Ø¨Ù‡ ÛŒÚ© ÙØ§ØµÙ„Ù‡
    return text

def normalize_url(url):
    """Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù† URL Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
    parsed = urlparse(url)
    clean_url = urlunparse(parsed._replace(query=""))
    return clean_url.strip().lower()

def is_similar_title(title1, title2, threshold=0.8):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¨Ø§Ù‡Øª Ø¯Ùˆ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø³Ø§Ø¯Ù‡"""
    from difflib import SequenceMatcher
    
    norm1 = normalize_text(title1)
    norm2 = normalize_text(title2)
    
    if not norm1 or not norm2:
        return False
    
    similarity = SequenceMatcher(None, norm1, norm2).ratio()
    return similarity >= threshold

# Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
# Ø¨Ù‡Ø¨ÙˆØ¯: Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ title ÛŒØ§ url Ùˆ agency

def save_news(news_items):
    db = SessionLocal()
    try:
        added_count = 0
        for item in news_items:
            # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù†ÙˆØ§Ù† Ù…Ø´Ø§Ø¨Ù‡ Ùˆ Ø¢Ú˜Ø§Ù†Ø³
            existing_news = db.query(News).filter(News.agency == item['agency']).all()
            
            is_duplicate = False
            for existing in existing_news:
                if is_similar_title(item['title'], existing.title):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                news = News(
                    title=item['title'],  # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ø§ØµÙ„ÛŒ
                    url=item['url'],      # Ø°Ø®ÛŒØ±Ù‡ URL Ø§ØµÙ„ÛŒ
                    agency=item['agency'],
                    published_at=item['published_at'],
                    summary=item['summary']
                )
                db.add(news)
                added_count += 1
                print(f"Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: {item['title'][:50]}...")
            else:
                print(f"Ø®Ø¨Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: {item['title'][:50]}...")
        
        db.commit()
        print(f"ØªØ¹Ø¯Ø§Ø¯ {added_count} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {len(news_items)} Ø®Ø¨Ø± Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø±: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    print("Ø´Ø±ÙˆØ¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±...")
    
    all_news = []
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± IRNA
    try:
        irna_news = fetch_irna_top_news()
        if irna_news:
            save_news(irna_news)
            all_news.extend(irna_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² IRNA Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± IRNA: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± BBC
    try:
        bbc_news = fetch_bbc_persian_news()
        if bbc_news:
            save_news(bbc_news)
            all_news.extend(bbc_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² BBC Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± BBC: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± IranIntl
    try:
        iranintl_news = fetch_iranintl_news()
        if iranintl_news:
            save_news(iranintl_news)
            all_news.extend(iranintl_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² IranIntl Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± IranIntl: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ISNA
    try:
        isna_news = fetch_isna_news()
        if isna_news:
            save_news(isna_news)
            all_news.extend(isna_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² ISNA Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± ISNA: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Tasnim
    try:
        tasnim_news = fetch_tasnim_news()
        if tasnim_news:
            save_news(tasnim_news)
            all_news.extend(tasnim_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² Tasnim Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± Tasnim: {e}")
    
    # Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
    agencies = {}
    for news in all_news:
        if news['agency'] not in agencies:
            agencies[news['agency']] = 0
        agencies[news['agency']] += 1
    
    print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:")
    for agency, count in agencies.items():
        print(f"  ğŸ“° {agency}: {count} Ø®Ø¨Ø±")
    
    print(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(all_news)}")
    print("ğŸ¯ Ø¹Ù…Ù„ÛŒØ§Øª Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ú©Ø§Ù…Ù„ Ø´Ø¯.") 