import requests
from bs4 import BeautifulSoup
from database import SessionLocal, News
import datetime
from datetime import timezone
from dateutil import parser as date_parser
from urllib.parse import urlparse, urlunparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from persian_summarizer import PersianSummarizer

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù…Ù‡Ù… Ø§Ø² IRNA Ø¨Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

def fetch_irna_top_news():
    url = 'https://www.irna.ir/'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    news_list = []
    summarizer = PersianSummarizer()
    all_news_items = []
    for item in soup.select('div.top-news a')[:10]:
        title = item.get_text(strip=True)
        link = item['href']
        if not link.startswith('http'):
            link = 'https://www.irna.ir' + link
        # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
        is_duplicate = False
        for existing in all_news_items:
            if existing['title'] == title or existing['url'] == link:
                is_duplicate = True
                break
        if not is_duplicate:
            all_news_items.append({'title': title, 'url': link})
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± IRNA: {len(all_news_items)}")
    for i, news_item in enumerate(all_news_items):
        print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± IRNA {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
        summary = ""
        try:
            summary = summarizer.get_news_summary(news_item['url'], news_item['title'])
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ IRNA: {e}")
            summary = ""
        today = datetime.datetime.now(timezone.utc)
        news_list.append({
            'title': news_item['title'],
            'url': news_item['url'],
            'agency': 'IRNA',
            'published_at': today,
            'summary': summary
        })
        print(f"Ø®Ø¨Ø± IRNA {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
    print(f"\nIRNA news count: {len(news_list)}")
    return news_list

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± BBC ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

def fetch_bbc_persian_news():
    try:
        url = 'https://www.bbc.com/persian/topics/ckdxnwvwwjnt'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        summarizer = PersianSummarizer()
        all_news_items = []
        selectors = [
            'ul[data-testid="topic-promos"] > li h2 a',
            'article h2 a',
            'div[data-testid="card-headline"] a'
        ]
        for selector in selectors:
            for a_tag in soup.select(selector):
                title = a_tag.get_text(strip=True)
                if not title:
                    continue
                link = a_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.bbc.com' + link
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 15:
                    break
            if len(all_news_items) >= 15:
                break
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± BBC: {len(all_news_items)}")
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± BBC {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                summary = summarizer.get_news_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ BBC: {e}")
                summary = ""
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'BBC',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± BBC {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        print(f"\nBBC news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± BBC: {e}")
        return []

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§ÛŒØ±Ø§Ù† Ø§ÛŒÙ†ØªØ±Ù†Ø´Ù†Ø§Ù„ Ø¨Ø§ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯

def fetch_iranintl_news():
    try:
        url = 'https://www.iranintl.com/iran'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        summarizer = PersianSummarizer()
        all_news_items = []
        selectors = [
            'article h3',
            'div.TopicCluster-module-scss-module__RZ03fG__featured article h3',
            'div.TopicCluster-module-scss-module__RZ03fG__additionalItem article h3',
            'div.topic__grid__item article h3'
        ]
        for selector in selectors:
            for title_tag in soup.select(selector):
                if not title_tag:
                    continue
                title = title_tag.get_text(strip=True)
                if not title:
                    continue
                article = title_tag.find_parent('article')
                if not article:
                    continue
                link_tag = article.select_one('a')
                if not link_tag:
                    continue
                link = link_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.iranintl.com' + link
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 15:
                    break
            if len(all_news_items) >= 15:
                break
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± IranIntl: {len(all_news_items)}")
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± IranIntl {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                summary = summarizer.get_news_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ IranIntl: {e}")
                summary = ""
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'IranIntl',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± IranIntl {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        print(f"\nIranIntl news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± IranIntl: {e}")
        return []

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ISNA
def fetch_isna_news():
    try:
        url = 'https://www.isna.ir/'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        summarizer = PersianSummarizer()
        all_news_items = []
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø±
        selectors = [
            'div.news-list h3 a',
            'div.top-news h3 a',
            'div.latest-news h3 a',
            'article h3 a'
        ]
        
        for selector in selectors:
            for a_tag in soup.select(selector):
                title = a_tag.get_text(strip=True)
                if not title or len(title) < 10:
                    continue
                link = a_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.isna.ir' + link
                
                # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 10:
                    break
            if len(all_news_items) >= 10:
                break
        
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± ISNA: {len(all_news_items)}")
        
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± ISNA {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                summary = summarizer.get_news_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ISNA: {e}")
                summary = ""
            
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'ISNA',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± ISNA {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        
        print(f"\nISNA news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ISNA: {e}")
        return []

# ØªØ§Ø¨Ø¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ØªØ³Ù†ÛŒÙ…
def fetch_tasnim_news():
    try:
        url = 'https://www.tasnimnews.com/'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        news_list = []
        summarizer = PersianSummarizer()
        all_news_items = []
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø±
        selectors = [
            'div.news-list h3 a',
            'div.top-news h3 a',
            'div.latest-news h3 a',
            'article h3 a',
            'div.news-item h3 a'
        ]
        
        for selector in selectors:
            for a_tag in soup.select(selector):
                title = a_tag.get_text(strip=True)
                if not title or len(title) < 10:
                    continue
                link = a_tag['href']
                if not link.startswith('http'):
                    link = 'https://www.tasnimnews.com' + link
                
                # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±ÛŒ
                is_duplicate = False
                for existing in all_news_items:
                    if existing['title'] == title or existing['url'] == link:
                        is_duplicate = True
                        break
                if not is_duplicate:
                    all_news_items.append({'title': title, 'url': link})
                if len(all_news_items) >= 10:
                    break
            if len(all_news_items) >= 10:
                break
        
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± Tasnim: {len(all_news_items)}")
        
        for i, news_item in enumerate(all_news_items):
            print(f"\nÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø®Ø¨Ø± Tasnim {i+1}/{len(all_news_items)}: {news_item['title'][:50]}...")
            summary = ""
            try:
                summary = summarizer.get_news_summary(news_item['url'], news_item['title'])
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Tasnim: {e}")
                summary = ""
            
            today = datetime.datetime.now(timezone.utc)
            news_list.append({
                'title': news_item['title'],
                'url': news_item['url'],
                'agency': 'Tasnim',
                'published_at': today,
                'summary': summary
            })
            print(f"Ø®Ø¨Ø± Tasnim {i+1} Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
        
        print(f"\nTasnim news count: {len(news_list)}")
        return news_list
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Tasnim: {e}")
        return []

def normalize_text(text):
    """Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
    if not text:
        return ""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ùˆ Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù†
    import re
    text = text.strip().lower()
    text = re.sub(r'[^\w\s]', '', text)  # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
    text = re.sub(r'\s+', ' ', text)     # ØªØ¨Ø¯ÛŒÙ„ Ú†Ù†Ø¯ÛŒÙ† ÙØ§ØµÙ„Ù‡ Ø¨Ù‡ ÛŒÚ© ÙØ§ØµÙ„Ù‡
    return text

def normalize_url(url):
    """Ù†Ø±Ù…Ø§Ù„ Ú©Ø±Ø¯Ù† URL Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡"""
    parsed = urlparse(url)
    clean_url = urlunparse(parsed._replace(query=""))
    return clean_url.strip().lower()

def is_similar_title(title1, title2, threshold=0.8):
    """Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¨Ø§Ù‡Øª Ø¯Ùˆ Ø¹Ù†ÙˆØ§Ù† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø³Ø§Ø¯Ù‡"""
    from difflib import SequenceMatcher
    
    norm1 = normalize_text(title1)
    norm2 = normalize_text(title2)
    
    if not norm1 or not norm2:
        return False
    
    similarity = SequenceMatcher(None, norm1, norm2).ratio()
    return similarity >= threshold

# Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø± Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡
# Ø¨Ù‡Ø¨ÙˆØ¯: Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ title ÛŒØ§ url Ùˆ agency

def save_news(news_items):
    db = SessionLocal()
    try:
        added_count = 0
        for item in news_items:
            # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù†ÙˆØ§Ù† Ù…Ø´Ø§Ø¨Ù‡ Ùˆ Ø¢Ú˜Ø§Ù†Ø³
            existing_news = db.query(News).filter(News.agency == item['agency']).all()
            
            is_duplicate = False
            for existing in existing_news:
                if is_similar_title(item['title'], existing.title):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                news = News(
                    title=item['title'],  # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ø§ØµÙ„ÛŒ
                    url=item['url'],      # Ø°Ø®ÛŒØ±Ù‡ URL Ø§ØµÙ„ÛŒ
                    agency=item['agency'],
                    published_at=item['published_at'],
                    summary=item['summary']
                )
                db.add(news)
                added_count += 1
                print(f"Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: {item['title'][:50]}...")
            else:
                print(f"Ø®Ø¨Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: {item['title'][:50]}...")
        
        db.commit()
        print(f"ØªØ¹Ø¯Ø§Ø¯ {added_count} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {len(news_items)} Ø®Ø¨Ø± Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø±: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    print("Ø´Ø±ÙˆØ¹ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±...")
    
    all_news = []
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± IRNA
    try:
        irna_news = fetch_irna_top_news()
        if irna_news:
            save_news(irna_news)
            all_news.extend(irna_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² IRNA Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± IRNA: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± BBC
    try:
        bbc_news = fetch_bbc_persian_news()
        if bbc_news:
            save_news(bbc_news)
            all_news.extend(bbc_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² BBC Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± BBC: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± IranIntl
    try:
        iranintl_news = fetch_iranintl_news()
        if iranintl_news:
            save_news(iranintl_news)
            all_news.extend(iranintl_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² IranIntl Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± IranIntl: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ISNA
    try:
        isna_news = fetch_isna_news()
        if isna_news:
            save_news(isna_news)
            all_news.extend(isna_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² ISNA Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± ISNA: {e}")
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Tasnim
    try:
        tasnim_news = fetch_tasnim_news()
        if tasnim_news:
            save_news(tasnim_news)
            all_news.extend(tasnim_news)
        else:
            print("Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø§Ø² Tasnim Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø®Ø¨Ø§Ø± Tasnim: {e}")
    
    # Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ
    agencies = {}
    for news in all_news:
        if news['agency'] not in agencies:
            agencies[news['agency']] = 0
        agencies[news['agency']] += 1
    
    print(f"\nğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ:")
    for agency, count in agencies.items():
        print(f"  ğŸ“° {agency}: {count} Ø®Ø¨Ø±")
    
    print(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(all_news)}")
    print("ğŸ¯ Ø¹Ù…Ù„ÛŒØ§Øª Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ú©Ø§Ù…Ù„ Ø´Ø¯.") 